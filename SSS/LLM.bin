LLM 기초

# LLM 이란 무엇인가?

GPT(GPT, Generative Pre-trained Transformer)

Generative : 생성하는
Pre-trained : 사전에 학습된
Transformer : 변환기

-> 인간과 유사한 텍스트를 생성할 수 있는 대규모 언어 모델

LLM (Large Language Model, 대규모 언어 모델)
다양한 형태의 언어 작업을 처리하고 이해할 수 있는 능력을 지닌 광범위한 용도로 사용되는 기술.

LLM은 '기초 모델(foundation model)'의 한 유형으로, 레이블이 지정되지 않은 자체 감독 데이터에 대해 사전 훈련된다.
-> 이러한 훈련을 통해서 모델은 데이터의 패턴을 학습하고 일반화 가능하게 적응 가능한 출력을 생성하는 방식을 개발한다.
LLM은 주로 텍스트와 텍스트와 유사한 항목(예시 : 코드)에 적용되며, 이는 모델이 더욱 다양한 문맥에서 유용하게 활용될 수 있음을 의미한다.

LLM은 책, 기사, 대화 등의 대규모 텍스트 데이터 세트를 학습하는데, 이러한 데이터가 모델의 크기를 수십 기가바이트 에서 페타바이트에 이르게 한다.

1GB -> 약 1억 7,800만 단어 저장 
1PB -> 약 1억 7,800만 * 100만

메개변수의 크기 
매개변수 : LLM 에서의 매개변수 모델의 학습능력과 복잡성을 결정하는 중요한 요소로, 모델이 다양한 언어 구조와 문맥을 이해하고 생성할 수 있게하는 내부규칙과 가중치, 편향을 나타낸다.

매개변수 많으면 많을수록 더 많은 정보를 저장하고, 더 정교한 패턴을 인식할 수 있다.

# 어떻게 작동하는가?

LLM = data + architecture + training

data : 책, 기사, 대화 등의 대규모 텍스트 데이터 세트
architecture : LLM 은 Transformer 아키텍쳐를 기반으로 함

Transformer = '어텐션 매커니즘'을 사용하여 문장의 각 단어 전체 문맥에서 어떤 의미를 가지는지를 파악
이를 통해 모델은 전체 텍스트를 보다 정확하게 이해하고, 관련된 응답을 생성할 수 있게함

training : 모델 훈련은 일반적으로 다음 단어 예측을 통해 이루어진다.

하늘은 붉다. ~> 하늘은 푸르다.
sky is blue

모델은 각 반복에서 내부 매개변수를 조정하면서 예측의 정확성을 향상시키는 방식으로 학습한다.

안정적인 문장 생성이 가능해지면, 모델을 보다 작고 구체적인 데이터 세트에서 미세 조정(fine-tuning)을 거쳐 특정 작업에 더 효과적인 모델을 만들 수 있다.

copilot, chat-gpt